{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('loan.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop nan\n",
    "* In this problem, dropping nan just makes 614 to 480, that is OK\n",
    "* But in lab5, dropping nan makes 10000 to 17000, that is unacceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Loan_ID\", axis=1, inplace=True) # this column is useless\n",
    "print(df.isnull().sum()) # Check how many nans there are in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task1 deal with NULL rows, you can either choose to drop them or replace them with mean or other value\n",
    "df = df.dropna(axis=0, how='any', inplace=False) # drop NaN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical features\n",
    "* Yaning Yang told me we could use dummy variables, but I didn't learn it well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task2 deal with categorical features\n",
    "df.Gender=df.Gender.map({'Male':1,'Female':0})\n",
    "df.Married=df.Married.map({'Yes':1,'No':0})\n",
    "df.Dependents=df.Dependents.map({'3+':3,'2':2,'1':1,'0':0})\n",
    "df.Education=df.Education.map({'Graduate':1,'Not Graduate':0})\n",
    "df.Self_Employed=df.Self_Employed.map({'Yes':1,'No':0})\n",
    "df.Property_Area=df.Property_Area.map({'Urban':2,'Semiurban':1,'Rural':0})\n",
    "df.Loan_Status=df.Loan_Status.map({'Y':1,'N':-1})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"Loan_Status\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[:,5], density = True, bins = 10, facecolor=\"blue\", edgecolor=\"black\")\n",
    "plt.xlabel('applicant income') \n",
    "plt.ylabel('frequency') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[:,6], density = True, bins = 10, facecolor=\"blue\", edgecolor=\"black\")\n",
    "plt.xlabel('coapplicant income') \n",
    "plt.ylabel('frequency') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[:,7], density = True, bins = 10, facecolor=\"blue\", edgecolor=\"black\")\n",
    "plt.xlabel('loan amount') \n",
    "plt.ylabel('frequency') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[:,8], density = True, bins = 10, facecolor=\"blue\", edgecolor=\"black\")\n",
    "plt.xlabel('loan amount term') \n",
    "plt.ylabel('frequency') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split the train set and the test set\n",
    "* normalization vs standardization\n",
    "  * The former one usually refers to min-max-normalization\n",
    "  * The latter one usually refers to Z-score-standardization\n",
    "  * distinction\n",
    "    * Normalization is used when the data doesn't have Gaussian distribution whereas Standardization is used on data having Gaussian distribution.\n",
    "    * Normalization scales in a range of [0,1] or [-1,1]. Standardization is not bounded by range.\n",
    "    * Normalization is highly affected by outliers. Standardization is slightly affected by outliers.\n",
    "    * Normalization is considered when the algorithms do not make assumptions about the data distribution. Standardization is used when algorithms make assumptions about the data distribution.\n",
    "* 先划分训练集测试集还是先归一化或标准化？\n",
    "  * 严格来说，应该先划分，否则测试集就用到了训练集的信息\n",
    "  * 在lab1和lab5中，我都是先划分再min-max归一化\n",
    "> ***Keep it simple, when you get too complex you forget the obvious***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法1：用min-max来标准化\n",
    "def min_max_normalization(data):\n",
    "    return (data - np.min(data, axis = 0))/(np.max(data, axis = 0) - np.min(data, axis = 0))\n",
    "    \n",
    "# 方法2：用Z-score来标准化\n",
    "def Z_score_standardization(data):\n",
    "    mu = np.mean(data, axis = 0)\n",
    "    sigma = np.std(data, axis = 0)\n",
    "    return (data - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task3 split the dataset into X_train, X_test, y_train, y_test\n",
    "\n",
    "# Shuffle the rows of the array\n",
    "np.random.seed(0)  # Set a seed for reproducibility\n",
    "df = df[np.random.permutation(df.shape[0])]\n",
    "\n",
    "# Split the array into two\n",
    "split = int(0.8 * df.shape[0])  # Calculate the index at which to split\n",
    "df1 = df[:split]  # Select rows 0 to split-1\n",
    "df2 = df[split:]  # Select rows split to end\n",
    "\n",
    "X_train = df1[:,0:11]\n",
    "Y_train = df1[:,11]\n",
    "X_train = min_max_normalization(X_train)\n",
    "X_train_design = np.insert(X_train, 0, 1, axis = 1)\n",
    "\n",
    "X_test = df2[:,0:11]\n",
    "Y_test = df2[:,11]\n",
    "X_test = min_max_normalization(X_test)\n",
    "X_test_design = np.insert(X_test, 0, 1, axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super parameter\n",
    "eta = 0.1\t\t\t\t\t    # 学习率\n",
    "epochs = 10 ** 4\t\t\t\t# 迭代上限\n",
    "epsilon = 10 ** -5\t\t\t\t# 梯度模长的上限"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task4 train your model and plot the loss curve of training\n",
    "\n",
    "# 初始点\n",
    "w_k = np.zeros(X_train_design.shape[1])\n",
    "\n",
    "# coherent to formula\n",
    "X = X_train_design\n",
    "y = Y_train\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def h(x):\n",
    "\treturn sigmoid(np.dot(x,w_k))\n",
    "\n",
    "def rhd(w):\n",
    "\treturn np.mean(np.log(1+np.exp(-y*(X@w))))\n",
    "\n",
    "def d_rhd(w):\n",
    "\treturn np.array([np.mean(-y*X[:, i]*np.exp(-y*(X@w))/(1+np.exp(-y*(X@w)))) for i in range(12)])\n",
    "\n",
    "loss = np.zeros(math.floor(epochs))\n",
    "for i in range(math.floor(epochs)): # 时间复杂度 O(n)\n",
    "\tloss[i] = rhd(w_k)\n",
    "\td_rhd_wk = d_rhd(w_k) # d_rhd_wk is the gradient of rhd\n",
    "\tif np.linalg.norm(d_rhd_wk) < epsilon:\n",
    "\t\tbreak\n",
    "\tw_k = w_k - eta * d_rhd_wk\n",
    "\n",
    "print(\"梯度下降法终止时的梯度的范数为：{}\".format(np.linalg.norm(d_rhd_wk)))\n",
    "plt.plot(loss[0:i+1])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"w: {}\".format(w_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of the train set\n",
    "y_prediction = w_k @ X_train_design.T\n",
    "y_prediction = np.where(y_prediction > 0, +1, -1)\n",
    "print(\"accuracy of the train set: {}\".format(1 - np.sum(np.abs(y_prediction - Y_train) / 2) / Y_train.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of the test set\n",
    "z = w_k @ X_test_design.T\n",
    "y_prediction = np.where(z > 0, +1, -1)\n",
    "print(\"accuracy of the test set: {}\".format(1 - np.sum(np.abs(y_prediction - Y_test) / 2) / Y_test.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## least square/pseudo inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate w\n",
    "pinvX = np.linalg.pinv(X_train_design)   # calculate pseudo inverse\n",
    "w = pinvX @ Y_train              \n",
    "print(\"w: {}\".format(w))\n",
    "\n",
    "# accuracy of train set\n",
    "z = w @ X_train_design.T\n",
    "y_prediction = np.where(z > 0, +1, -1)\n",
    "print(\"accuracy of train set: {}\".format(1 - np.sum(np.abs(y_prediction - Y_train) / 2) / Y_train.shape[0]))\n",
    "\n",
    "# accuracy of test set\n",
    "z = w @ X_test_design.T\n",
    "y_prediction = np.where(z > 0, +1, -1)\n",
    "print(\"accuracy of test set: {}\".format(1 - np.sum(np.abs(y_prediction - Y_test) / 2) / Y_test.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用sklearn实现least square和Logistic regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### least square/pseudo inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# linear regression\n",
    "Least_Square = LinearRegression().fit(X_train, Y_train)\n",
    "\n",
    "# print coef and intercept\n",
    "w_LS = Least_Square.coef_\n",
    "b_LS = Least_Square.intercept_\n",
    "print(\"w: {}\".format(Least_Square.coef_))\n",
    "print(\"b: {}\".format(Least_Square.intercept_))\n",
    "\n",
    "# accuracy of train set\n",
    "y_prediction = Least_Square.predict(X_train)\n",
    "y_prediction = np.where(y_prediction > 0, +1, -1)\n",
    "print(\"accuracy of train set: {}\".format(1 - np.sum(np.abs(y_prediction - Y_train) / 2) / Y_train.shape[0]))\n",
    "\n",
    "# accuracy of test set\n",
    "y_prediction = Least_Square.predict(X_test)\n",
    "y_prediction = np.where(y_prediction > 0, +1, -1)\n",
    "print(\"accuracy of test set: {}\".format(1 - np.sum(np.abs(y_prediction - Y_test) / 2) / Y_test.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# logistic regression\n",
    "logistic_regression = LogisticRegression(penalty='none', random_state=0).fit(X_train, Y_train)\n",
    "\n",
    "# print coef and intercept\n",
    "w_logistic_regression = logistic_regression.coef_\n",
    "b_logistic_regression = logistic_regression.intercept_\n",
    "print(\"w: {}\".format(logistic_regression.coef_))\n",
    "print(\"b: {}\".format(logistic_regression.intercept_))\n",
    "\n",
    "# accuracy of train set\n",
    "y_prediction = logistic_regression.predict(X_train)\n",
    "print(\"accuracy of train set: {}\".format(1 - np.sum(np.abs(y_prediction - Y_train) / 2) / Y_train.shape[0]))\n",
    "\n",
    "# accuracy of test set\n",
    "y_prediction = logistic_regression.predict(X_test)\n",
    "print(\"accuracy of test set: {}\".format(1 - np.sum(np.abs(y_prediction - Y_test) / 2) / Y_test.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "* $epoch = 10^4$时，正确率已经到达极限了\n",
    "  * 正确率的极限只有80%左右\n",
    "  * 要想更好，需要用特征转换等方法\n",
    "* w中绝对值最大的竟然是Property_Area，这是不是地域歧视啊；绝对值最小的是feature 6\n",
    "* 类别不是特别不平衡的时候不要用$N+/N-$代替1/2，否则会过拟合"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf245e2d5e4a2aef58e3292d21e566cf0ee03e2e9151002f5de995cbc6bcfd67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

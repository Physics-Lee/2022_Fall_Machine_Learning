{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in common use\n",
    "def min_max_normalization(data):\n",
    "    return (data - np.min(data, axis = 0))/(np.max(data, axis = 0) - np.min(data, axis = 0))\n",
    "\n",
    "def concatenate_two_1D_array(X1,X2):\n",
    "    return np.concatenate((X1.reshape(len(X1),1),X2.reshape(len(X2),1)),axis=1)\n",
    "\n",
    "def sort_by_row_in_2D_array_descending(X,row_number):\n",
    "    ind = np.argsort(-X[:,row_number])\n",
    "    return X[ind,:]\n",
    "\n",
    "def select_feature(X,selected_feature):\n",
    "    return X[:,selected_feature]\n",
    "\n",
    "def accuracy(y,y_pred):\n",
    "    return sum(y_pred == y)/len(y_pred == y)\n",
    "\n",
    "def save_ndarray_to_csv(X,filename):\n",
    "    X = pd.DataFrame(X)\n",
    "    X.to_csv(\"{}.csv\".format(filename), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split categorical feature and continuous feature\n",
    "# judge\n",
    "def Is_Continuous(X_train, unique_value_threshold):\n",
    "    return np.array([len(np.unique(X_train[:,i])) > unique_value_threshold for i in range(X_train.shape[1])])\n",
    "\n",
    "# check len unique\n",
    "def check_len_unique(X_train):\n",
    "    len_unique = np.zeros(X_train.shape[1])\n",
    "    for i in range(X_train.shape[1]):\n",
    "        len_unique[i] = len(np.unique(X_train[:, i]))\n",
    "    return len_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELIEF-F\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def diff(x_1,x_2,is_categorical):\n",
    "    if is_categorical == True: # categorical\n",
    "        if x_1 == x_2:\n",
    "            return 0\n",
    "        if x_1 != x_2:\n",
    "            return 1\n",
    "    if is_categorical == False: # continuous\n",
    "        return np.abs(x_1 - x_2)\n",
    "\n",
    "def reliefF(X_train, y_train, n_neighbors, is_categorical):\n",
    "    \n",
    "    # prepare\n",
    "    n_samples, n_features = X_train.shape\n",
    "    weight_of_feature = np.zeros(n_features)\n",
    "\n",
    "    # get the frequency of each label in y_train\n",
    "    counts = np.bincount(y_train)\n",
    "    frequency = counts / y_train.shape\n",
    "    \n",
    "    # use NearestNeighbors of sklearn to get enough neighbors\n",
    "    n_multiple = 10 # With lab5's ball-like bad data, 10x is enough\n",
    "    neigh = NearestNeighbors(n_neighbors = n_neighbors * len(counts) * n_multiple + 1, metric='minkowski', p = 1) # len(counts) = 4\n",
    "    neigh.fit(X_train)\n",
    "    _, hit_miss_indices = neigh.kneighbors(X_train)\n",
    "    hit_miss_indices = hit_miss_indices[:, 1:]\n",
    "    \n",
    "    # for each sample i\n",
    "    for i in range(n_samples):\n",
    "\n",
    "        # get hit indices, the amount equals n_neighbors\n",
    "        hit_indices = hit_miss_indices[i][y_train[hit_miss_indices[i]] == y_train[i]]\n",
    "        if len(hit_indices) < n_neighbors:\n",
    "            raise ValueError(\"length of hit_indices is less than n_neighbors\")\n",
    "        else:\n",
    "            hit_indices = hit_indices[0:n_neighbors]\n",
    "        \n",
    "        # get miss indices, the amount equals n_neighbors for each miss class\n",
    "        miss_indices = hit_miss_indices[i][y_train[hit_miss_indices[i]] != y_train[i]]        \n",
    "        miss_label = y_train[miss_indices]\n",
    "        miss_ind_and_label = np.concatenate((miss_indices.reshape(len(miss_indices),1),miss_label.reshape(len(miss_label),1)),axis=1)\n",
    "        label = np.unique(miss_ind_and_label[:,1])\n",
    "        count = np.zeros(len(label))\n",
    "        delete = []        \n",
    "        for k in range(miss_ind_and_label.shape[0]): # for each row k of miss_ind_and_label\n",
    "            for l in range(len(label)): # len(label) is the number of class in y_train minus 1\n",
    "                if miss_ind_and_label[k,1] == label[l]:\n",
    "                    count[l] += 1\n",
    "                    if count[l] >= n_neighbors + 1:\n",
    "                        delete.append(k)\n",
    "        miss_ind_and_label = np.delete(miss_ind_and_label, delete, axis = 0)\n",
    "        if miss_ind_and_label.shape[0] != (len(label))*n_neighbors:\n",
    "            raise ValueError(\"length of miss_indices is less than n_neighbors\")\n",
    "        miss_indices = miss_ind_and_label[:,0]\n",
    "\n",
    "        # for each feature j\n",
    "        for j in range(n_features):\n",
    "            for hit_idx in hit_indices:      \n",
    "                weight_of_feature[j] -= diff(X_train[i, j],X_train[hit_idx, j],is_categorical[j]) / n_neighbors\n",
    "            for miss_idx in miss_indices:\n",
    "                weight_of_miss = frequency[y_train[miss_idx]]/(1 - frequency[y_train[i]]) # calculate weighted average of miss classes\n",
    "                weight_of_feature[j] += diff(X_train[i, j],X_train[miss_idx, j],is_categorical[j]) / n_neighbors * weight_of_miss\n",
    "\n",
    "    return weight_of_feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = pd.read_csv('train_feature_preprocessed.csv')\n",
    "test_feature = pd.read_csv('test_feature_preprocessed.csv')\n",
    "train_label = pd.read_csv('Dataset/train_label.csv')\n",
    "train_feature = train_feature.drop('Unnamed: 0',axis=1)\n",
    "test_feature = test_feature.drop('Unnamed: 0',axis=1)\n",
    "train_feature = train_feature.to_numpy() \n",
    "test_feature = test_feature.to_numpy()\n",
    "train_label = train_label.to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "X = train_feature\n",
    "y = train_label\n",
    "k = 5 # a hyper-parameter\n",
    "stratified_kf = StratifiedKFold(n_splits = k,shuffle = True,random_state = 0)\n",
    "n_neigh_RELIEF = 10 # a hyper-parameter\n",
    "number_of_features_choosed_by_RELIEF = 10 # a hyper-parameter\n",
    "result_sum = np.zeros((number_of_features_choosed_by_RELIEF*n_neigh_RELIEF,))\n",
    "for train_index, val_index in stratified_kf.split(X, y):\n",
    "\n",
    "    # split\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # reshape\n",
    "    y_train = y_train.reshape(len(y_train),)\n",
    "    y_val = y_val.reshape(len(y_val),)\n",
    "\n",
    "    # number of samples and features\n",
    "    n_sample = X_train.shape[0]\n",
    "    n_feature = X_train.shape[1]\n",
    "\n",
    "    # min-max-normalization\n",
    "    X_train = min_max_normalization(X_train)\n",
    "    X_val = min_max_normalization(X_val)\n",
    "\n",
    "    # split\n",
    "    threshold_of_unique_value = 23 # a hyper-parameter\n",
    "    is_continuous = Is_Continuous(X_train, threshold_of_unique_value)\n",
    "    is_categorical = ~is_continuous\n",
    "\n",
    "    # RELIEF-F\n",
    "    weight_of_feature = reliefF(X_train, y_train, n_neigh_RELIEF, is_categorical)\n",
    "\n",
    "    # knn    \n",
    "    result = np.zeros((number_of_features_choosed_by_RELIEF*n_neigh_RELIEF,3))\n",
    "    count_knn = 0\n",
    "    for i in range(1,number_of_features_choosed_by_RELIEF,1):\n",
    "\n",
    "        # select top i features\n",
    "        weight_of_feature_concatenate = concatenate_two_1D_array(weight_of_feature,np.arange(120))\n",
    "        weight_of_feature_concatenate = sort_by_row_in_2D_array_descending(weight_of_feature_concatenate,0)\n",
    "        selected_feature = weight_of_feature_concatenate[0:i,1]\n",
    "        selected_feature = selected_feature.astype(int)\n",
    "        X_train_RELIEF = select_feature(X_train,selected_feature)\n",
    "        X_val_RELIEF = select_feature(X_val,selected_feature)\n",
    "\n",
    "        # perform knn\n",
    "        for n_neigh_knn in np.arange(1,n_neigh_RELIEF + 1,1): # a hyper-parameter\n",
    "            knn = KNeighborsClassifier(n_neighbors=n_neigh_knn,p = 1,metric = 'minkowski')\n",
    "            knn.fit(X_train_RELIEF, y_train)\n",
    "            y_val_pred = knn.predict(X_val_RELIEF)\n",
    "            result[count_knn,0] = i\n",
    "            result[count_knn,1] = n_neigh_knn\n",
    "            result[count_knn,2] = accuracy(y_val,y_val_pred)\n",
    "            count_knn += 1\n",
    "    result_sum += result[:,2]\n",
    "result_sum = result_sum / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_ndarray_to_csv(result_sum,'result_5_new')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf245e2d5e4a2aef58e3292d21e566cf0ee03e2e9151002f5de995cbc6bcfd67"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

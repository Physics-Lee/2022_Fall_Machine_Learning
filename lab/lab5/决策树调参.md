# 只控制ccp_alpha，“证明”这30个feature比120个feature好

首先，我把10000个sample按照8：2划分为训练集、测试集。分层抽样。

然后，分别进行归一化，没有泄露测试集的信息。

随后，我用训练集中的8000个sample进行特征选择，选出了30个特征（RELIEF的前二十名和mutual information的前十名的并集）。这一步我完全没有泄露测试集的信息。



对于30个feature:

找出合适的ccp_alpha的区间且对每个ccp_alpha做100次实验求平均。

当训练集正确率在80%-90%时，测试集的平均准确率为25.41%。

数据见v2。



对于120个feature:

操作同上。

当训练集正确率在80%-90%时，测试集的平均准确率为24.90%。

数据见v2。



这足以“证明”这30个feature比120个feature好。



代码如下

```python
# compare 30 features with 120 features
my_range = np.arange(0.00055,0.00075,0.00001)
count = 0
result_ccp_alpha = np.zeros((len(my_range),3))
for ccp_alpha in my_range:
    n_loop = 10
    y_train_acc = 0
    y_val_acc = 0
    for j in range(n_loop):
        # fit
        DT = DecisionTreeClassifier(ccp_alpha=ccp_alpha,random_state=0)
        DT.fit(X_train, y_train)
        # train
        y_train_pred = DT.predict(X_train)
        y_train_acc += accuracy(y_train,y_train_pred)
        # val
        y_val_pred = DT.predict(X_val)
        y_val_acc += accuracy(y_val,y_val_pred)
    y_train_acc /= n_loop
    y_val_acc /= n_loop
    result_ccp_alpha[count,0] = ccp_alpha
    result_ccp_alpha[count,1] = y_train_acc
    result_ccp_alpha[count,2] = y_val_acc
    count += 1
```





随后，我指定了`random_state = 66`

然后，在验证集上找出训练集准确率在80%-90%时最好的ccp_alpha（0.00039），验证集准确率为25.8%。(可以重复)

测试集准确率为25.3%

其实到这一步我已经可以交作业了，因为准确率已经超过了25%。

代码如下

```python
# find the most suitable ccp_alpha
X_train_RELIEF_I_mutual, X_val_RELIEF_I_mutual, y_train, y_val = train_test_split(X_train_RELIEF_I_mutual, y_train, test_size=0.25, random_state=42, stratify=y_train)
my_range = np.arange(0.00025,0.00045,0.00001)
count = 0
result_ccp_alpha = np.zeros((len(my_range),3))
for ccp_alpha in my_range:

    # fit
    DT = DecisionTreeClassifier(ccp_alpha=ccp_alpha,random_state=66)
    DT.fit(X_train_RELIEF_I_mutual, y_train)

    # train
    y_train_pred = DT.predict(X_train_RELIEF_I_mutual)
    y_train_acc = accuracy(y_train,y_train_pred)

    # val
    y_val_pred = DT.predict(X_val_RELIEF_I_mutual)
    y_val_acc = accuracy(y_val,y_val_pred)

    # preserve
    result_ccp_alpha[count,0] = ccp_alpha
    result_ccp_alpha[count,1] = y_train_acc
    result_ccp_alpha[count,2] = y_val_acc
    count = count + 1

DT = DecisionTreeClassifier(ccp_alpha=0.00039,random_state=66)
DT.fit(X_train_RELIEF_I_mutual, y_train)
y_test_pred = DT.predict(X_test_RELIEF_I_mutual)
accuracy(y_test,y_test_pred)
```



# 精细调参

调参顺序

* criterion
* max_depth
* min_samples_split
* min_samples_leaf
* grid，联合调2、3、4



第一轮调参过后，交叉验证的准确率达到了0.265125，==测试集准确率达到了0.268==
